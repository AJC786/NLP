{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Information Extraction_Ajay_03.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM+ZV3+yrrhuQF4cvd6f4pP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"x7gRv9xw1kNO"},"source":["Information Extraction\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3w5YWmQ31ndM","executionInfo":{"status":"ok","timestamp":1618144201522,"user_tz":-330,"elapsed":1390,"user":{"displayName":"Ajay Chaurasiya","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrJfkNWkC-o3-Csu6TarxnsV7-Y7dgEQDBz1BL1Q=s64","userId":"12602377681777354203"}},"outputId":"5c68120b-9b62-4eb3-adf1-ffc627e33110"},"source":["# import the necessary libraries\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","from nltk import CFG\n","import matplotlib.pyplot as plt\n","import string\n","import re\n","from nltk.tokenize import word_tokenize \n","from nltk import pos_tag\n","from tkinter import *\n","\n","  \n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag, ne_chunk\n","  \n","def named_entity_recognition(text):\n","    # tokenize the text\n","    word_tokens = word_tokenize(text)\n","  \n","    # part of speech tagging of words\n","    word_pos = pos_tag(word_tokens)\n","  \n","    # tree of word entities\n","    print(ne_chunk(word_pos))\n","  \n","text = 'Bill works for GeeksforGeeks so he went to Delhi for a meetup.'\n","named_entity_recognition(text)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n","(S\n","  (PERSON Bill/NNP)\n","  works/VBZ\n","  for/IN\n","  (ORGANIZATION GeeksforGeeks/NNP)\n","  so/RB\n","  he/PRP\n","  went/VBD\n","  to/TO\n","  (GPE Delhi/NNP)\n","  for/IN\n","  a/DT\n","  meetup/NN\n","  ./.)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"la_X-zI56KCj","executionInfo":{"status":"ok","timestamp":1618144181458,"user_tz":-330,"elapsed":1239,"user":{"displayName":"Ajay Chaurasiya","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrJfkNWkC-o3-Csu6TarxnsV7-Y7dgEQDBz1BL1Q=s64","userId":"12602377681777354203"}},"outputId":"53fdaca4-e5c3-462b-bcc4-1e20ddf4a6b3"},"source":["from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","  \n","# convert text into word_tokens with their tags\n","def pos_tagging(text):\n","    word_tokens = word_tokenize(text)\n","    return pos_tag(word_tokens)\n","  \n","pos_tagging('You just gave me a scare')"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('You', 'PRP'),\n"," ('just', 'RB'),\n"," ('gave', 'VBD'),\n"," ('me', 'PRP'),\n"," ('a', 'DT'),\n"," ('scare', 'NN')]"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jBwM2K2S5-MJ","executionInfo":{"status":"ok","timestamp":1618144211505,"user_tz":-330,"elapsed":1062,"user":{"displayName":"Ajay Chaurasiya","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrJfkNWkC-o3-Csu6TarxnsV7-Y7dgEQDBz1BL1Q=s64","userId":"12602377681777354203"}},"outputId":"c13afe45-ea33-411c-9065-07cf37be996d"},"source":["# download the tagset \n","nltk.download('tagsets')\n","  \n","# extract information about the tag\n","nltk.help.upenn_tagset('NN')"],"execution_count":31,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package tagsets to /root/nltk_data...\n","[nltk_data]   Package tagsets is already up-to-date!\n","NN: noun, common, singular or mass\n","    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n","    investment slide humour falloff slick wind hyena override subhumanity\n","    machinist ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ExCvBy936y0","executionInfo":{"status":"ok","timestamp":1618144361165,"user_tz":-330,"elapsed":982,"user":{"displayName":"Ajay Chaurasiya","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrJfkNWkC-o3-Csu6TarxnsV7-Y7dgEQDBz1BL1Q=s64","userId":"12602377681777354203"}},"outputId":"22b56024-f401-4155-994b-02f4e46f08c2"},"source":["from nltk.tree import *\n","from collections import defaultdict\n","from nltk.tokenize import word_tokenize \n","from nltk import pos_tag\n","  \n","# define chunking function with text and regular\n","# expression representing grammar as parameter\n","def chunking(text, grammar):\n","    word_tokens = word_tokenize(text)\n","  \n","    # label words with part of speech\n","    word_pos = pos_tag(word_tokens)\n","  \n","    # create a chunk parser using grammar\n","    chunkParser = nltk.RegexpParser(grammar)\n","  \n","    # test it on the list of word tokens with tagged pos\n","    tree = chunkParser.parse(word_pos)\n","\n","    for subtree in tree.subtrees():\n","        print(subtree)\n","    \n","      \n","sentence = 'the little yellow bird is flying in the sky'\n","grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n","chunking(sentence, grammar)\n","      "],"execution_count":37,"outputs":[{"output_type":"stream","text":["(S\n","  (NP the/DT little/JJ yellow/JJ bird/NN)\n","  is/VBZ\n","  flying/VBG\n","  in/IN\n","  (NP the/DT sky/NN))\n","(NP the/DT little/JJ yellow/JJ bird/NN)\n","(NP the/DT sky/NN)\n"],"name":"stdout"}]}]}